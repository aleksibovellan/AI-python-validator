Loading dataset...
Loading tokenizer...
Preparing datasets...

Creating augmented dataset for training...
--- Example 1 ---
Original code:
def send_text_card(self, agent_id, user_ids, title, description, url, btntxt='详情',
                       party_ids='', tag_ids=''):
        
        return self.send(
            agent_id,
          

Corrupted code:
def send_text_card(self, agent_id, user_ids, title, description, url, btntxt='详情',
                       party_ids='', tag_ids=''):
        
        return self.send(
            agent_id,
          

Tokens are different: True

--- Example 2 ---
Original code:
def get_group(self, name, user_name=None):
        
        self.project_service.set_auth(self._token_project)
        return self.project_service.get_group(name, user_name)

Corrupted code:
def get_groupself, name, user_name=None:
        
        self.project_service.set_authself._token_project
        return self.project_service.get_groupname, user_name

Tokens are different: True

--- Example 3 ---
Original code:
def dispatch(self, test=False):  
        
        if not self.new_to_dispatch:
            raise DispatcherError("Dispatcher cannot dispatch, "
                                  "because no configura

Corrupted code:
def dispatch(self, test=False):  
        
        if not self.new_to_dispatch:
            raise DispatcherError(Dispatcher cannot dispatch, 
                                  because no configuratio

Tokens are different: True


Creating augmented dataset for validation...
--- Example 1 ---
Original code:
def update_location(self, text=''):
        
        self.text_project_name.setEnabled(self.radio_new_dir.isChecked())
        name = self.text_project_name.text().strip()

        if name and se

Corrupted code:
def update_location(self, text=''):
        
        self.text_project_name.setEnabled(self.radio_new_dir.isChecked())
        name = self.text_project_name.text().strip()

        if name and se

Tokens are different: True

--- Example 2 ---
Original code:
def create_pretraining_model(nlp, tok2vec):
    
    output_size = nlp.vocab.vectors.data.shape[1]
    output_layer = chain(
        LN(Maxout(300, pieces=3)), Affine(output_size, drop_factor=0.0)
   

Corrupted code:
def create_pretraining_model(nlp, tok2vec)
    
    output_size = nlp.vocab.vectors.data.shape[1]
    output_layer = chain(
        LN(Maxout(300, pieces=3)), Affine(output_size, drop_factor=0.0)
    

Tokens are different: True

--- Example 3 ---
Original code:
def dump(obj, fp=None, indent=None, sort_keys=False, **kw):
    

    if fp:
        iterable = YAMLEncoder(indent=indent, sort_keys=sort_keys, **kw).iterencode(obj)
        for chunk in iterable:
   

Corrupted code:
def dump(obj, fp=None, indent=None, sort_keys=False, **kw):
    

    if fp:
        iterable = YAMLEncoder(indent=indent, sort_keys=sort_keys, **kw).iterencode(obj)
        for chunk in iterable:
   

Tokens are different: True


Training set label distribution: [24010 24005]
Validation set label distribution: [6000 6000]

Tokenizing training dataset...
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 48015/48015 [00:15<00:00, 3069.49 examples/s]
Tokenizing validation dataset...
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 12000/12000 [00:03<00:00, 3114.94 examples/s]

Tokenized training set labels: [1, 1, 1, 0, 1, 1, 1, 1, 0, 1]
Tokenized validation set labels: [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]

Loading model...
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Initializing Trainer...

Starting training...
{'loss': 0.6795, 'grad_norm': 4.695720672607422, 'learning_rate': 1.9888925913584364e-05, 'epoch': 0.02}                                            
{'loss': 0.4511, 'grad_norm': 2.841805934906006, 'learning_rate': 1.9777851827168722e-05, 'epoch': 0.03}                                            
{'loss': 0.4413, 'grad_norm': 15.558694839477539, 'learning_rate': 1.9666777740753084e-05, 'epoch': 0.05}                                           
{'loss': 0.4259, 'grad_norm': 2.7393932342529297, 'learning_rate': 1.9555703654337446e-05, 'epoch': 0.07}                                           
{'loss': 0.3812, 'grad_norm': 12.853519439697266, 'learning_rate': 1.9444629567921804e-05, 'epoch': 0.08}                                           
{'loss': 0.3492, 'grad_norm': 6.430034637451172, 'learning_rate': 1.9333555481506166e-05, 'epoch': 0.1}                                             
{'loss': 0.2985, 'grad_norm': 5.901570796966553, 'learning_rate': 1.9222481395090528e-05, 'epoch': 0.12}                                            
{'loss': 0.2998, 'grad_norm': 3.8199362754821777, 'learning_rate': 1.9111407308674887e-05, 'epoch': 0.13}                                           
{'loss': 0.3184, 'grad_norm': 18.689908981323242, 'learning_rate': 1.900033322225925e-05, 'epoch': 0.15}                                            
{'loss': 0.2353, 'grad_norm': 3.0864598751068115, 'learning_rate': 1.888925913584361e-05, 'epoch': 0.17}                                            
{'loss': 0.3039, 'grad_norm': 13.14169979095459, 'learning_rate': 1.877818504942797e-05, 'epoch': 0.18}                                             
{'loss': 0.2627, 'grad_norm': 39.95235824584961, 'learning_rate': 1.866711096301233e-05, 'epoch': 0.2}                                              
{'loss': 0.3205, 'grad_norm': 6.849061965942383, 'learning_rate': 1.8556036876596693e-05, 'epoch': 0.22}                                            
{'loss': 0.2752, 'grad_norm': 5.861017227172852, 'learning_rate': 1.844496279018105e-05, 'epoch': 0.23}                                             
{'loss': 0.2611, 'grad_norm': 5.643673896789551, 'learning_rate': 1.8333888703765413e-05, 'epoch': 0.25}                                            
{'loss': 0.2426, 'grad_norm': 6.401151180267334, 'learning_rate': 1.8222814617349775e-05, 'epoch': 0.27}                                            
{'loss': 0.2402, 'grad_norm': 6.109279155731201, 'learning_rate': 1.8111740530934133e-05, 'epoch': 0.28}                                            
{'loss': 0.2463, 'grad_norm': 13.14252758026123, 'learning_rate': 1.8000666444518495e-05, 'epoch': 0.3}                                             
{'loss': 0.2119, 'grad_norm': 4.738692283630371, 'learning_rate': 1.7889592358102857e-05, 'epoch': 0.32}                                            
{'loss': 0.2317, 'grad_norm': 16.251605987548828, 'learning_rate': 1.777851827168722e-05, 'epoch': 0.33}                                            
{'loss': 0.2655, 'grad_norm': 14.889127731323242, 'learning_rate': 1.7667444185271578e-05, 'epoch': 0.35}                                           
{'loss': 0.2405, 'grad_norm': 8.35344409942627, 'learning_rate': 1.755637009885594e-05, 'epoch': 0.37}                                              
{'loss': 0.2309, 'grad_norm': 7.528780460357666, 'learning_rate': 1.7445296012440298e-05, 'epoch': 0.38}                                            
{'loss': 0.2335, 'grad_norm': 6.501984596252441, 'learning_rate': 1.733422192602466e-05, 'epoch': 0.4}                                              
{'loss': 0.2528, 'grad_norm': 6.319031238555908, 'learning_rate': 1.722314783960902e-05, 'epoch': 0.42}                                             
{'loss': 0.2227, 'grad_norm': 13.983373641967773, 'learning_rate': 1.711207375319338e-05, 'epoch': 0.43}                                            
{'loss': 0.2171, 'grad_norm': 10.223512649536133, 'learning_rate': 1.7000999666777742e-05, 'epoch': 0.45}                                           
{'loss': 0.2129, 'grad_norm': 1.278738260269165, 'learning_rate': 1.6889925580362104e-05, 'epoch': 0.47}                                            
{'loss': 0.2259, 'grad_norm': 2.1673989295959473, 'learning_rate': 1.6778851493946466e-05, 'epoch': 0.48}                                           
{'loss': 0.2419, 'grad_norm': 7.387078285217285, 'learning_rate': 1.6667777407530824e-05, 'epoch': 0.5}                                             
{'loss': 0.2099, 'grad_norm': 1.8555717468261719, 'learning_rate': 1.6556703321115186e-05, 'epoch': 0.52}                                           
{'loss': 0.2273, 'grad_norm': 29.394733428955078, 'learning_rate': 1.6445629234699545e-05, 'epoch': 0.53}                                           
{'loss': 0.2631, 'grad_norm': 2.075181007385254, 'learning_rate': 1.6334555148283907e-05, 'epoch': 0.55}                                            
{'loss': 0.1961, 'grad_norm': 0.6504018306732178, 'learning_rate': 1.622348106186827e-05, 'epoch': 0.57}                                            
{'loss': 0.2221, 'grad_norm': 3.128098964691162, 'learning_rate': 1.6112406975452627e-05, 'epoch': 0.58}                                            
{'loss': 0.2072, 'grad_norm': 1.3433537483215332, 'learning_rate': 1.600133288903699e-05, 'epoch': 0.6}                                             
{'loss': 0.2023, 'grad_norm': 1.3457335233688354, 'learning_rate': 1.589025880262135e-05, 'epoch': 0.62}                                            
{'loss': 0.2045, 'grad_norm': 9.759361267089844, 'learning_rate': 1.5779184716205713e-05, 'epoch': 0.63}                                            
{'loss': 0.2093, 'grad_norm': 5.228992938995361, 'learning_rate': 1.566811062979007e-05, 'epoch': 0.65}                                             
{'loss': 0.2154, 'grad_norm': 11.59041976928711, 'learning_rate': 1.5557036543374433e-05, 'epoch': 0.67}                                            
{'loss': 0.2225, 'grad_norm': 1.8571008443832397, 'learning_rate': 1.544596245695879e-05, 'epoch': 0.68}                                            
{'loss': 0.2108, 'grad_norm': 5.899705410003662, 'learning_rate': 1.5334888370543153e-05, 'epoch': 0.7}                                             
{'loss': 0.2041, 'grad_norm': 1.8099009990692139, 'learning_rate': 1.5223814284127514e-05, 'epoch': 0.72}                                           
{'loss': 0.1989, 'grad_norm': 49.907291412353516, 'learning_rate': 1.5112740197711875e-05, 'epoch': 0.73}                                           
{'loss': 0.208, 'grad_norm': 1.675952672958374, 'learning_rate': 1.5001666111296236e-05, 'epoch': 0.75}                                             
{'loss': 0.1851, 'grad_norm': 22.82166290283203, 'learning_rate': 1.4890592024880598e-05, 'epoch': 0.77}                                            
{'loss': 0.1942, 'grad_norm': 5.400355339050293, 'learning_rate': 1.4779517938464958e-05, 'epoch': 0.78}                                            
{'loss': 0.2013, 'grad_norm': 12.968480110168457, 'learning_rate': 1.466844385204932e-05, 'epoch': 0.8}                                             
{'loss': 0.1838, 'grad_norm': 3.7787044048309326, 'learning_rate': 1.455736976563368e-05, 'epoch': 0.82}                                            
{'loss': 0.191, 'grad_norm': 2.4010722637176514, 'learning_rate': 1.4446295679218038e-05, 'epoch': 0.83}                                            
{'loss': 0.2082, 'grad_norm': 4.513698101043701, 'learning_rate': 1.43352215928024e-05, 'epoch': 0.85}                                              
{'loss': 0.1785, 'grad_norm': 0.5062581896781921, 'learning_rate': 1.422414750638676e-05, 'epoch': 0.87}                                            
{'loss': 0.2303, 'grad_norm': 0.938386082649231, 'learning_rate': 1.4113073419971122e-05, 'epoch': 0.88}                                            
{'loss': 0.16, 'grad_norm': 6.245334148406982, 'learning_rate': 1.4001999333555482e-05, 'epoch': 0.9}                                               
{'loss': 0.1956, 'grad_norm': 1.2931299209594727, 'learning_rate': 1.3890925247139844e-05, 'epoch': 0.92}                                           
{'loss': 0.1894, 'grad_norm': 2.477250814437866, 'learning_rate': 1.3779851160724205e-05, 'epoch': 0.93}                                            
{'loss': 0.2562, 'grad_norm': 6.900676250457764, 'learning_rate': 1.3668777074308566e-05, 'epoch': 0.95}                                            
{'loss': 0.1796, 'grad_norm': 0.4829646646976471, 'learning_rate': 1.3557702987892927e-05, 'epoch': 0.97}                                           
{'loss': 0.2092, 'grad_norm': 1.4384803771972656, 'learning_rate': 1.3446628901477285e-05, 'epoch': 0.98}                                           
{'loss': 0.2117, 'grad_norm': 13.657689094543457, 'learning_rate': 1.3335554815061647e-05, 'epoch': 1.0}                                            
{'eval_loss': 0.20226170122623444, 'eval_accuracy': 0.9321666666666667, 'eval_runtime': 726.2077, 'eval_samples_per_second': 16.524, 'eval_steps_per_second': 1.033, 'epoch': 1.0}                                                                                                                      
{'loss': 0.1764, 'grad_norm': 0.5463197231292725, 'learning_rate': 1.3224480728646007e-05, 'epoch': 1.02}                                           
{'loss': 0.2005, 'grad_norm': 1.6252806186676025, 'learning_rate': 1.3113406642230369e-05, 'epoch': 1.03}                                           
{'loss': 0.184, 'grad_norm': 4.178804397583008, 'learning_rate': 1.300233255581473e-05, 'epoch': 1.05}                                              
{'loss': 0.2078, 'grad_norm': 5.0407795906066895, 'learning_rate': 1.2891258469399091e-05, 'epoch': 1.07}                                           
{'loss': 0.2095, 'grad_norm': 1.4167112112045288, 'learning_rate': 1.2780184382983451e-05, 'epoch': 1.08}                                           
{'loss': 0.1854, 'grad_norm': 12.788725852966309, 'learning_rate': 1.2669110296567813e-05, 'epoch': 1.1}                                            
{'loss': 0.2055, 'grad_norm': 1.47492516040802, 'learning_rate': 1.2558036210152173e-05, 'epoch': 1.12}                                             
{'loss': 0.1561, 'grad_norm': 7.036576271057129, 'learning_rate': 1.2446962123736532e-05, 'epoch': 1.13}                                            
{'loss': 0.1959, 'grad_norm': 53.17727279663086, 'learning_rate': 1.2335888037320894e-05, 'epoch': 1.15}                                            
{'loss': 0.1934, 'grad_norm': 2.3625028133392334, 'learning_rate': 1.2224813950905254e-05, 'epoch': 1.17}                                           
{'loss': 0.1759, 'grad_norm': 1.898954153060913, 'learning_rate': 1.2113739864489616e-05, 'epoch': 1.18}                                            
{'loss': 0.1661, 'grad_norm': 2.387976884841919, 'learning_rate': 1.2002665778073976e-05, 'epoch': 1.2}                                             
{'loss': 0.2181, 'grad_norm': 20.91474151611328, 'learning_rate': 1.1891591691658338e-05, 'epoch': 1.22}                                            
{'loss': 0.1813, 'grad_norm': 10.066427230834961, 'learning_rate': 1.1780517605242698e-05, 'epoch': 1.23}                                           
{'loss': 0.1798, 'grad_norm': 0.9053809642791748, 'learning_rate': 1.166944351882706e-05, 'epoch': 1.25}                                            
{'loss': 0.1958, 'grad_norm': 1.8050764799118042, 'learning_rate': 1.155836943241142e-05, 'epoch': 1.27}                                            
{'loss': 0.1923, 'grad_norm': 1.764426827430725, 'learning_rate': 1.1447295345995779e-05, 'epoch': 1.28}                                            
{'loss': 0.1544, 'grad_norm': 44.22332763671875, 'learning_rate': 1.133622125958014e-05, 'epoch': 1.3}                                              
{'loss': 0.1981, 'grad_norm': 2.2869482040405273, 'learning_rate': 1.12251471731645e-05, 'epoch': 1.32}                                             
{'loss': 0.161, 'grad_norm': 13.386419296264648, 'learning_rate': 1.1114073086748863e-05, 'epoch': 1.33}                                            
{'loss': 0.1717, 'grad_norm': 2.3158376216888428, 'learning_rate': 1.1002999000333223e-05, 'epoch': 1.35}                                           
{'loss': 0.1951, 'grad_norm': 37.17953872680664, 'learning_rate': 1.0891924913917585e-05, 'epoch': 1.37}                                            
{'loss': 0.2033, 'grad_norm': 3.8760437965393066, 'learning_rate': 1.0780850827501945e-05, 'epoch': 1.38}                                           
{'loss': 0.1905, 'grad_norm': 5.9137654304504395, 'learning_rate': 1.0669776741086307e-05, 'epoch': 1.4}                                            
{'loss': 0.2071, 'grad_norm': 3.74668550491333, 'learning_rate': 1.0558702654670667e-05, 'epoch': 1.42}                                             
{'loss': 0.2022, 'grad_norm': 0.5965479612350464, 'learning_rate': 1.0447628568255026e-05, 'epoch': 1.43}                                           
{'loss': 0.1891, 'grad_norm': 10.424213409423828, 'learning_rate': 1.0336554481839387e-05, 'epoch': 1.45}                                           
{'loss': 0.1866, 'grad_norm': 0.6307903528213501, 'learning_rate': 1.0225480395423748e-05, 'epoch': 1.47}                                           
{'loss': 0.1892, 'grad_norm': 2.6469016075134277, 'learning_rate': 1.011440630900811e-05, 'epoch': 1.48}                                            
{'loss': 0.2145, 'grad_norm': 1.2185150384902954, 'learning_rate': 1.000333222259247e-05, 'epoch': 1.5}                                             
{'loss': 0.1697, 'grad_norm': 2.4547741413116455, 'learning_rate': 9.892258136176832e-06, 'epoch': 1.52}                                            
{'loss': 0.1702, 'grad_norm': 0.7133760452270508, 'learning_rate': 9.781184049761192e-06, 'epoch': 1.53}                                            
{'loss': 0.1881, 'grad_norm': 1.5252280235290527, 'learning_rate': 9.670109963345552e-06, 'epoch': 1.55}                                            
{'loss': 0.1727, 'grad_norm': 0.7855762839317322, 'learning_rate': 9.559035876929912e-06, 'epoch': 1.57}                                            
{'loss': 0.1493, 'grad_norm': 14.379171371459961, 'learning_rate': 9.447961790514274e-06, 'epoch': 1.58}                                            
{'loss': 0.1766, 'grad_norm': 2.426164150238037, 'learning_rate': 9.336887704098634e-06, 'epoch': 1.6}                                              
{'loss': 0.1979, 'grad_norm': 0.8930273056030273, 'learning_rate': 9.225813617682994e-06, 'epoch': 1.62}                                            
{'loss': 0.2015, 'grad_norm': 1.486868143081665, 'learning_rate': 9.114739531267356e-06, 'epoch': 1.63}                                             
{'loss': 0.1946, 'grad_norm': 1.452285885810852, 'learning_rate': 9.003665444851716e-06, 'epoch': 1.65}                                             
{'loss': 0.2016, 'grad_norm': 1.410959005355835, 'learning_rate': 8.892591358436078e-06, 'epoch': 1.67}                                             
{'loss': 0.1454, 'grad_norm': 0.9414543509483337, 'learning_rate': 8.781517272020439e-06, 'epoch': 1.68}                                            
{'loss': 0.1812, 'grad_norm': 9.86506462097168, 'learning_rate': 8.670443185604799e-06, 'epoch': 1.7}                                               
{'loss': 0.1756, 'grad_norm': 4.698597431182861, 'learning_rate': 8.55936909918916e-06, 'epoch': 1.72}                                              
{'loss': 0.1699, 'grad_norm': 2.801201581954956, 'learning_rate': 8.44829501277352e-06, 'epoch': 1.73}                                              
{'loss': 0.1756, 'grad_norm': 1.788917064666748, 'learning_rate': 8.337220926357881e-06, 'epoch': 1.75}                                             
{'loss': 0.1883, 'grad_norm': 3.5145530700683594, 'learning_rate': 8.226146839942241e-06, 'epoch': 1.77}                                            
{'loss': 0.1572, 'grad_norm': 1.6764237880706787, 'learning_rate': 8.115072753526603e-06, 'epoch': 1.78}                                            
{'loss': 0.1897, 'grad_norm': 2.921103000640869, 'learning_rate': 8.003998667110963e-06, 'epoch': 1.8}                                              
{'loss': 0.1582, 'grad_norm': 1.9383970499038696, 'learning_rate': 7.892924580695325e-06, 'epoch': 1.82}                                            
{'loss': 0.1554, 'grad_norm': 1.4708260297775269, 'learning_rate': 7.781850494279685e-06, 'epoch': 1.83}                                            
{'loss': 0.1963, 'grad_norm': 1.3516873121261597, 'learning_rate': 7.670776407864046e-06, 'epoch': 1.85}                                            
{'loss': 0.2044, 'grad_norm': 3.309786558151245, 'learning_rate': 7.559702321448407e-06, 'epoch': 1.87}                                             
{'loss': 0.1857, 'grad_norm': 0.8776149153709412, 'learning_rate': 7.448628235032768e-06, 'epoch': 1.88}                                            
{'loss': 0.1546, 'grad_norm': 0.5775908827781677, 'learning_rate': 7.337554148617129e-06, 'epoch': 1.9}                                             
{'loss': 0.1839, 'grad_norm': 0.9713972210884094, 'learning_rate': 7.226480062201489e-06, 'epoch': 1.92}                                            
{'loss': 0.1652, 'grad_norm': 0.21560855209827423, 'learning_rate': 7.11540597578585e-06, 'epoch': 1.93}                                            
{'loss': 0.1693, 'grad_norm': 1.0469930171966553, 'learning_rate': 7.004331889370211e-06, 'epoch': 1.95}                                            
{'loss': 0.1591, 'grad_norm': 2.323002338409424, 'learning_rate': 6.893257802954572e-06, 'epoch': 1.97}                                             
{'loss': 0.1543, 'grad_norm': 0.8825967311859131, 'learning_rate': 6.782183716538931e-06, 'epoch': 1.98}                                            
{'loss': 0.2161, 'grad_norm': 1.606351613998413, 'learning_rate': 6.671109630123292e-06, 'epoch': 2.0}                                              
{'eval_loss': 0.18517480790615082, 'eval_accuracy': 0.937, 'eval_runtime': 745.164, 'eval_samples_per_second': 16.104, 'eval_steps_per_second': 1.006, 'epoch': 2.0}                                                                                                                                    
{'loss': 0.1551, 'grad_norm': 0.4143798351287842, 'learning_rate': 6.560035543707653e-06, 'epoch': 2.02}                                            
{'loss': 0.1597, 'grad_norm': 1.3534762859344482, 'learning_rate': 6.4489614572920145e-06, 'epoch': 2.03}                                           
{'loss': 0.1581, 'grad_norm': 0.9489620327949524, 'learning_rate': 6.3378873708763755e-06, 'epoch': 2.05}                                           
{'loss': 0.1476, 'grad_norm': 2.4800641536712646, 'learning_rate': 6.226813284460736e-06, 'epoch': 2.07}                                            
{'loss': 0.1692, 'grad_norm': 1.2333576679229736, 'learning_rate': 6.115739198045097e-06, 'epoch': 2.08}                                            
{'loss': 0.1801, 'grad_norm': 1.3185365200042725, 'learning_rate': 6.004665111629458e-06, 'epoch': 2.1}                                             
{'loss': 0.1674, 'grad_norm': 1.6525288820266724, 'learning_rate': 5.893591025213819e-06, 'epoch': 2.12}                                            
{'loss': 0.1731, 'grad_norm': 3.8765170574188232, 'learning_rate': 5.782516938798178e-06, 'epoch': 2.13}                                            
{'loss': 0.1561, 'grad_norm': 0.5330677032470703, 'learning_rate': 5.671442852382539e-06, 'epoch': 2.15}                                            
{'loss': 0.1678, 'grad_norm': 2.076759099960327, 'learning_rate': 5.5603687659669e-06, 'epoch': 2.17}                                               
{'loss': 0.153, 'grad_norm': 2.777038812637329, 'learning_rate': 5.449294679551261e-06, 'epoch': 2.18}                                              
{'loss': 0.1551, 'grad_norm': 5.809299468994141, 'learning_rate': 5.338220593135622e-06, 'epoch': 2.2}                                              
{'loss': 0.1633, 'grad_norm': 2.727037191390991, 'learning_rate': 5.2271465067199825e-06, 'epoch': 2.22}                                            
{'loss': 0.1864, 'grad_norm': 0.5248985290527344, 'learning_rate': 5.1160724203043435e-06, 'epoch': 2.23}                                           
{'loss': 0.1283, 'grad_norm': 1.5394811630249023, 'learning_rate': 5.0049983338887046e-06, 'epoch': 2.25}                                           
{'loss': 0.1747, 'grad_norm': 0.7095007300376892, 'learning_rate': 4.893924247473065e-06, 'epoch': 2.27}                                            
{'loss': 0.1766, 'grad_norm': 2.610952377319336, 'learning_rate': 4.782850161057426e-06, 'epoch': 2.28}                                             
{'loss': 0.1504, 'grad_norm': 2.1609344482421875, 'learning_rate': 4.671776074641787e-06, 'epoch': 2.3}                                             
{'loss': 0.1673, 'grad_norm': 0.42288917303085327, 'learning_rate': 4.560701988226147e-06, 'epoch': 2.32}                                           
{'loss': 0.1684, 'grad_norm': 3.5755302906036377, 'learning_rate': 4.449627901810508e-06, 'epoch': 2.33}                                            
{'loss': 0.1724, 'grad_norm': 1.4083832502365112, 'learning_rate': 4.338553815394868e-06, 'epoch': 2.35}                                            
{'loss': 0.1737, 'grad_norm': 0.657244086265564, 'learning_rate': 4.227479728979229e-06, 'epoch': 2.37}                                             
{'loss': 0.1631, 'grad_norm': 5.245730876922607, 'learning_rate': 4.11640564256359e-06, 'epoch': 2.38}                                              
{'loss': 0.1611, 'grad_norm': 5.447000503540039, 'learning_rate': 4.005331556147951e-06, 'epoch': 2.4}                                              
{'loss': 0.1806, 'grad_norm': 1.534390926361084, 'learning_rate': 3.8942574697323116e-06, 'epoch': 2.42}                                            
{'loss': 0.1569, 'grad_norm': 0.6840555667877197, 'learning_rate': 3.7831833833166726e-06, 'epoch': 2.43}                                           
{'loss': 0.1815, 'grad_norm': 0.7629179954528809, 'learning_rate': 3.6721092969010332e-06, 'epoch': 2.45}                                           
{'loss': 0.1791, 'grad_norm': 0.8104057312011719, 'learning_rate': 3.561035210485394e-06, 'epoch': 2.47}                                            
{'loss': 0.2052, 'grad_norm': 0.7374016046524048, 'learning_rate': 3.449961124069755e-06, 'epoch': 2.48}                                            
{'loss': 0.172, 'grad_norm': 19.906827926635742, 'learning_rate': 3.3388870376541155e-06, 'epoch': 2.5}                                             
{'loss': 0.1826, 'grad_norm': 0.9656530618667603, 'learning_rate': 3.2278129512384765e-06, 'epoch': 2.52}                                           
{'loss': 0.1619, 'grad_norm': 0.4136241674423218, 'learning_rate': 3.116738864822837e-06, 'epoch': 2.53}                                            
{'loss': 0.193, 'grad_norm': 48.77901840209961, 'learning_rate': 3.0056647784071978e-06, 'epoch': 2.55}                                             
{'loss': 0.2046, 'grad_norm': 2.4149677753448486, 'learning_rate': 2.8945906919915584e-06, 'epoch': 2.57}                                           
{'loss': 0.1768, 'grad_norm': 5.008800029754639, 'learning_rate': 2.7835166055759194e-06, 'epoch': 2.58}                                            
{'loss': 0.1492, 'grad_norm': 0.7580201625823975, 'learning_rate': 2.67244251916028e-06, 'epoch': 2.6}                                              
{'loss': 0.1451, 'grad_norm': 0.816305935382843, 'learning_rate': 2.561368432744641e-06, 'epoch': 2.62}                                             
{'loss': 0.1918, 'grad_norm': 3.6912310123443604, 'learning_rate': 2.4502943463290017e-06, 'epoch': 2.63}                                           
{'loss': 0.1938, 'grad_norm': 7.276769161224365, 'learning_rate': 2.3392202599133623e-06, 'epoch': 2.65}                                            
{'loss': 0.18, 'grad_norm': 1.7978098392486572, 'learning_rate': 2.228146173497723e-06, 'epoch': 2.67}                                              
{'loss': 0.166, 'grad_norm': 1.992771863937378, 'learning_rate': 2.117072087082084e-06, 'epoch': 2.68}                                              
{'loss': 0.1417, 'grad_norm': 2.766056776046753, 'learning_rate': 2.005998000666445e-06, 'epoch': 2.7}                                              
{'loss': 0.1716, 'grad_norm': 4.2189106941223145, 'learning_rate': 1.8949239142508056e-06, 'epoch': 2.72}                                           
{'loss': 0.1443, 'grad_norm': 0.46198439598083496, 'learning_rate': 1.7838498278351662e-06, 'epoch': 2.73}                                          
{'loss': 0.1682, 'grad_norm': 0.4656407833099365, 'learning_rate': 1.672775741419527e-06, 'epoch': 2.75}                                            
{'loss': 0.2192, 'grad_norm': 2.7587203979492188, 'learning_rate': 1.5617016550038877e-06, 'epoch': 2.77}                                           
{'loss': 0.1808, 'grad_norm': 25.623184204101562, 'learning_rate': 1.4506275685882485e-06, 'epoch': 2.78}                                           
{'loss': 0.1583, 'grad_norm': 0.4038783013820648, 'learning_rate': 1.339553482172609e-06, 'epoch': 2.8}                                             
{'loss': 0.1383, 'grad_norm': 1.1442205905914307, 'learning_rate': 1.2284793957569701e-06, 'epoch': 2.82}                                           
{'loss': 0.1806, 'grad_norm': 0.48396074771881104, 'learning_rate': 1.1174053093413307e-06, 'epoch': 2.83}                                          
{'loss': 0.1907, 'grad_norm': 0.6640769243240356, 'learning_rate': 1.0063312229256916e-06, 'epoch': 2.85}                                           
{'loss': 0.1772, 'grad_norm': 0.8865000605583191, 'learning_rate': 8.952571365100522e-07, 'epoch': 2.87}                                            
{'loss': 0.1292, 'grad_norm': 3.9560728073120117, 'learning_rate': 7.841830500944131e-07, 'epoch': 2.88}                                            
{'loss': 0.1625, 'grad_norm': 0.5239164233207703, 'learning_rate': 6.731089636787738e-07, 'epoch': 2.9}                                             
{'loss': 0.1891, 'grad_norm': 4.895458221435547, 'learning_rate': 5.620348772631346e-07, 'epoch': 2.92}                                             
{'loss': 0.1487, 'grad_norm': 3.8854405879974365, 'learning_rate': 4.5096079084749534e-07, 'epoch': 2.93}                                           
{'loss': 0.1956, 'grad_norm': 2.98923659324646, 'learning_rate': 3.3988670443185606e-07, 'epoch': 2.95}                                             
{'loss': 0.157, 'grad_norm': 0.5476495027542114, 'learning_rate': 2.2881261801621683e-07, 'epoch': 2.97}                                            
{'loss': 0.1489, 'grad_norm': 0.8579846620559692, 'learning_rate': 1.1773853160057759e-07, 'epoch': 2.98}                                           
{'loss': 0.141, 'grad_norm': 0.5732464790344238, 'learning_rate': 6.664445184938354e-09, 'epoch': 3.0}                                              
{'eval_loss': 0.18200400471687317, 'eval_accuracy': 0.9385, 'eval_runtime': 705.852, 'eval_samples_per_second': 17.001, 'eval_steps_per_second': 1.063, 'epoch': 3.0}                                                                                                                                   
{'train_runtime': 30614.0478, 'train_samples_per_second': 4.705, 'train_steps_per_second': 0.294, 'train_loss': 0.2004567445827354, 'epoch': 3.0}   
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 9003/9003 [8:30:14<00:00,  3.40s/it]

Evaluating model...
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 750/750 [11:34<00:00,  1.08it/s]
Evaluation results: {'eval_loss': 0.18200400471687317, 'eval_accuracy': 0.9385, 'eval_runtime': 695.4209, 'eval_samples_per_second': 17.256, 'eval_steps_per_second': 1.078, 'epoch': 3.0}

Saving model and tokenizer...

Loading trained model for evaluation...
Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.

Classification Results:
1. Correct Code: LABEL_1 (Confidence: 1.00)
2. Incorrect Code: LABEL_0 (Confidence: 1.00)
3. Correct Code: LABEL_1 (Confidence: 0.97)
4. Incorrect Code: LABEL_0 (Confidence: 1.00)


Classification result for code_snippet_1: [{'label': 'LABEL_1', 'score': 0.9680263996124268}
Classification result for code_snippet_2: [{'label': 'LABEL_0', 'score': 0.9998996257781982}


**************


If no errors were printed: training and evaluation was completed successfully!
You can now proceed to launch the Gradio interface script and use it with your web browser :)
